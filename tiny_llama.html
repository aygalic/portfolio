<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Tiny Llama</title><link rel="icon" href="/portfolio/favicon.ico" data-next-head=""/><link rel="preload" href="/portfolio/_next/static/css/42fac297191103bc.css" as="style"/><link rel="stylesheet" href="/portfolio/_next/static/css/42fac297191103bc.css" data-n-g=""/><link rel="preload" href="/portfolio/_next/static/css/0638347c1ec532ad.css" as="style"/><link rel="stylesheet" href="/portfolio/_next/static/css/0638347c1ec532ad.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/portfolio/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/portfolio/_next/static/chunks/webpack-a2fe7aa5da4fd8ed.js" defer=""></script><script src="/portfolio/_next/static/chunks/framework-052b50cd3d4947f2.js" defer=""></script><script src="/portfolio/_next/static/chunks/main-cfbcfe6489f952bc.js" defer=""></script><script src="/portfolio/_next/static/chunks/pages/_app-4d6a7da869db1ef0.js" defer=""></script><script src="/portfolio/_next/static/chunks/121-8182aff827244f67.js" defer=""></script><script src="/portfolio/_next/static/chunks/pages/tiny_llama-658deb935fa325be.js" defer=""></script><script src="/portfolio/_next/static/0_oSwcvFA7iMDCrzAxKnB/_buildManifest.js" defer=""></script><script src="/portfolio/_next/static/0_oSwcvFA7iMDCrzAxKnB/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Home_container__d256j"><header class="header_header__OaHfl"><nav class="header_nav__q1mtj"><a class="header_homeLink__zzaFd" href="/portfolio">Home</a></nav></header><main class="Home_main__VkIEL"><h1 class="Home_title__hYX6j">Minimal LLM implementation.</h1>Built with Llama<p class="Home_text__FLP25">Using a quantized Llama 3.2 with 1B parameters. This demo simply summarize random wikipedia articles. In order to meet the requirement of file size for git-lfs, the model Has been quantized using <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, reducing its size to ~700MB. The model was then deployed on <a href="https://huggingface.co/spaces/aygalic/tiny-llama">Hugging Face Spaces</a> through github action.<br/>Despite its relatively small size the performance are quite poor since the model is running (for free) on 2 vCPU cores.<br/>You can find the code for the API and quantization script on <a href="https://github.com/aygalic/llama-summarizer">github</a> as well as all the machinery for hugging face deployment.</p><div class="Home_subframe__T0sZp"><h2>Random Wikipedia Article</h2><button>New Article</button><br/><div></div></div><div class="Home_hidden__RZvqD"><div class="Home_subframe__T0sZp"><h3>Article Content</h3><div>Loading...</div></div></div><div class="Home_hidden__RZvqD"><div class="Home_subframe__T0sZp"><button disabled="">Summarize with LLM</button></div></div><div class="Home_subframe__T0sZp"><button disabled="">Get Summary</button></div></main><footer class="footer_footer__e_ilT"><nav class="footer_nav__ea2xt"><a class="footer_homeLink__fI7wA" href="https://github.com/aygalic">github</a><a class="footer_homeLink__fI7wA" href="https://www.linkedin.com/in/aygalic/">LinkedIn</a></nav></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/tiny_llama","query":{},"buildId":"0_oSwcvFA7iMDCrzAxKnB","assetPrefix":"/portfolio","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>